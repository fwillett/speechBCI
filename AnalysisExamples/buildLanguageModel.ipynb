{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Prerequisite\n",
    "\n",
    "To run this notebook, you need to build the decoder binaries and runtime first. Please refer to [README.md](../LanguageModelDecoder/README.md) for more details.\n",
    "\n",
    "You will need at least **230GB** of free disk space and **100GB** of RAM to run this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Prepare language model training corpus. \n",
    "\n",
    "The training corpus should be a text file with one sentence per line. Here we use [OpenWebText2](https://openwebtext2.readthedocs.io/en/latest/) as an example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Download the OpenWebText2 corpus\n",
    "\n",
    "CORPUS_DIR=lm_corpus\n",
    "mkdir -p $CORPUS_DIR\n",
    "# If the download URL does not work, you can find the latest one at https://openwebtext2.readthedocs.io/en/latest/\n",
    "wget https://mystic.the-eye.eu/public/AI/pile_preliminary_components/openwebtext2.jsonl.zst.tar -O $CORPUS_DIR/openwebtext2.jsonl.zst.tar\n",
    "cd $CORPUS_DIR\n",
    "tar -xvf openwebtext2.jsonl.zst.tar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to concatenate all the text files into one big file.\n",
    "Make sure you have python libraries `zstandard`, `jsonlines`, and `tqdm` installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import zstandard\n",
    "import json\n",
    "import jsonlines\n",
    "import io\n",
    "import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def json_serial(obj):\n",
    "    \"\"\"JSON serializer for objects not serializable by default json code\"\"\"\n",
    "\n",
    "    if isinstance(obj, (datetime.datetime,)):\n",
    "        return obj.isoformat()\n",
    "    raise TypeError (\"Type %s not serializable\" % type(obj))\n",
    "\n",
    "# Modified version of lm_dataformat Archive for single file.\n",
    "class Archive:\n",
    "    def __init__(self, file_path, compression_level=3):\n",
    "        self.file_path = file_path\n",
    "        dir_name = os.path.dirname(file_path)\n",
    "        if dir_name:\n",
    "            os.makedirs(dir_name, exist_ok=True)\n",
    "        self.fh = open(self.file_path, 'wb')\n",
    "        self.cctx = zstandard.ZstdCompressor(level=compression_level)\n",
    "        self.compressor = self.cctx.stream_writer(self.fh)\n",
    "\n",
    "    def add_data(self, data, meta={}):\n",
    "        self.compressor.write(json.dumps({'text': data, 'meta': meta}, default=json_serial).encode('UTF-8') + b'\\n')\n",
    "\n",
    "    def commit(self):\n",
    "        self.compressor.flush(zstandard.FLUSH_FRAME)\n",
    "        self.fh.flush()\n",
    "        self.fh.close()\n",
    "\n",
    "# Modified version of lm_dataformat Reader with self.fh set, allowing peeking for tqdm.\n",
    "class Reader:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def read_jsonl(self, file, get_meta=False, autojoin_paragraphs=True, para_joiner='\\n\\n'):\n",
    "        with open(file, 'rb') as fh:\n",
    "            self.fh = fh\n",
    "            cctx = zstandard.ZstdDecompressor()\n",
    "            reader = io.BufferedReader(cctx.stream_reader(fh))\n",
    "            rdr = jsonlines.Reader(reader)\n",
    "            for ob in rdr:\n",
    "                # naive jsonl where each object is just the string itself, with no meta. For legacy compatibility.\n",
    "                if isinstance(ob, str):\n",
    "                    assert not get_meta\n",
    "                    yield ob\n",
    "                    continue\n",
    "\n",
    "                text = ob['text']\n",
    "\n",
    "                if autojoin_paragraphs and isinstance(text, list):\n",
    "                    text = para_joiner.join(text)\n",
    "\n",
    "                if get_meta:\n",
    "                    yield text, (ob['meta'] if 'meta' in ob else {})\n",
    "                else:\n",
    "                    yield text\n",
    "\n",
    "lm_corpus_dir = 'lm_corpus'\n",
    "merged_text_path = 'lm_corpus/openwebtext2.txt'\n",
    "output = open(merged_text_path, 'w')\n",
    "\n",
    "files = sorted(glob.glob(os.path.join(lm_corpus_dir, \"*jsonl.zst\")))\n",
    "for file_path in tqdm(files, dynamic_ncols=True):\n",
    "    print(file_path)\n",
    "    reader = Reader()\n",
    "    for document in tqdm(reader.read_jsonl(file_path)):\n",
    "        output.write(document)\n",
    "        output.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Download CMU dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "wget https://github.com/Alexir/CMUdict/raw/master/cmudict-0.7b -O lm_corpus/cmudict.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build language model\n",
    "\n",
    "Build a 3-gram language model based on the OpenWebText2 corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/oak/stanford/groups/shenoy/stfan/code/speechBCI/LanguageModelDecoder/examples/speech/s0\n",
      "/oak/stanford/groups/shenoy/stfan/code/speechBCI/AnalysisExamples/lm_model/data/local/lm\n",
      "Prune LM with threshold 1e-9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "set -xe\n",
    "\n",
    "LM_ROOT=../LanguageModelDecoder/examples/speech/s0/\n",
    "LM_CORPUS_DIR=$PWD/lm_corpus\n",
    "LM_MODEL_DIR=$PWD/lm_model\n",
    "\n",
    "cd $LM_ROOT\n",
    "echo $PWD\n",
    ". path.sh\n",
    "\n",
    "# First step is formatting the text corpus.\n",
    "mkdir -p $LM_MODEL_DIR/data/local/lm_data\n",
    "python local/format_lm_data.py \\\n",
    "    --input_text $LM_CORPUS_DIR/openwebtext2.txt \\\n",
    "    --output_text $LM_MODEL_DIR/data/local/lm_data/corpus.txt \\\n",
    "    --dict $LM_CORPUS_DIR/cmudict.txt \\\n",
    "    --unk\n",
    "\n",
    "# Build the LM\n",
    "dict_type=phn\n",
    "lm_order=3\n",
    "prune_threshold=1e-9\n",
    "local/build_lm.sh \\\n",
    "    $LM_MODEL_DIR/data/local/lm_data/corpus.txt \\\n",
    "    $LM_MODEL_DIR/data/local/lm \\\n",
    "    $dict_type \\\n",
    "    $lm_order \\\n",
    "    $prune_threshold \\\n",
    "    $LM_CORPUS_DIR/cmudict.txt\n",
    "\n",
    "# Optionally, if you have 1TB of RAM, you can build a 5-gram LM\n",
    "#dict_type=phn\n",
    "#lm_order=5\n",
    "#prune_threshold=4e-11\n",
    "#local/build_lm.sh \\\n",
    "#    $LM_MODEL_DIR/data/local/lm_data/corpus.txt \\\n",
    "#    $LM_MODEL_DIR/data/local/lm \\\n",
    "#    $dict_type \\\n",
    "#    $lm_order \\\n",
    "#    $prune_threshold \\\n",
    "#    $LM_CORPUS_DIR/cmudict.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build WFST decoder graph\n",
    "\n",
    "Convert the previous 3-gram language model into a WFST decoder graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "LM_ROOT=../LanguageModelDecoder/examples/speech/s0/\n",
    "LM_MODEL_DIR=$PWD/lm_model\n",
    "use_all_phones=1\n",
    "dict_type=phn\n",
    "sil_prob=0.9\n",
    "\n",
    "cd $LM_ROOT\n",
    ". path.sh\n",
    "\n",
    "# Prepare L.fst\n",
    "local/prepare_dict_ctc.sh $LM_MODEL_DIR/data/local/lm $LM_MODEL_DIR/data/local/dict_phn $use_all_phones\n",
    "tools/fst/ctc_compile_dict_token.sh --dict-type $dict_type --sil-prob $sil_prob \\\n",
    "    $LM_MODEL_DIR/data/local/dict_phn $LM_MODEL_DIR/data/local/lang_phn_tmp $LM_MODEL_DIR/data/lang_phn\n",
    "\n",
    "# Build TLG decoding graph\n",
    "tools/fst/make_tlg.sh $LM_MODEL_DIR/data/local/lm $LM_MODEL_DIR/data/lang_phn $LM_MODEL_DIR/data/lang_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test loading the deocder graph. Make sure you have [NeuralDecoder](../NeuralDecoder) installed before running this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import lm_decoder\n",
    "\n",
    "import neuralDecoder.utils.lmDecoderUtils as lmDecoderUtils\n",
    "\n",
    "ngramDecoder = lmDecoderUtils.build_lm_decoder(\n",
    "    'lm_model/data/lang_test'\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.9",
   "language": "python",
   "name": "py3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
