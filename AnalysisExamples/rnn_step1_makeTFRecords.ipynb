{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5522653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# formats the competitionData into tfRecords for RNN training, including blockwise feature normalization\n",
    "# speechBCI 까지 온 상태 (ls 치면 AnalysisExamples 나오게)\n",
    "# baseDir = \"/home/s2/nlp002/nlp/speechBCI\"\n",
    "baseDir = \"/oak/stanford/groups/henderj/fwillett/speechPaperRelease_08_20\"\n",
    "\n",
    "import os\n",
    "\n",
    "os.makedirs(baseDir + \"/derived/tfRecords\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "292d4581",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-04 18:06:29.596757: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from g2p_en import G2p\n",
    "import re\n",
    "\n",
    "import pathlib\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "PHONE_DEF = [\n",
    "    'AA', 'AE', 'AH', 'AO', 'AW',\n",
    "    'AY', 'B',  'CH', 'D', 'DH',\n",
    "    'EH', 'ER', 'EY', 'F', 'G',\n",
    "    'HH', 'IH', 'IY', 'JH', 'K',\n",
    "    'L', 'M', 'N', 'NG', 'OW',\n",
    "    'OY', 'P', 'R', 'S', 'SH',\n",
    "    'T', 'TH', 'UH', 'UW', 'V',\n",
    "    'W', 'Y', 'Z', 'ZH'\n",
    "]\n",
    "\n",
    "PHONE_DEF_SIL = [\n",
    "    'AA', 'AE', 'AH', 'AO', 'AW',\n",
    "    'AY', 'B',  'CH', 'D', 'DH',\n",
    "    'EH', 'ER', 'EY', 'F', 'G',\n",
    "    'HH', 'IH', 'IY', 'JH', 'K',\n",
    "    'L', 'M', 'N', 'NG', 'OW',\n",
    "    'OY', 'P', 'R', 'S', 'SH',\n",
    "    'T', 'TH', 'UH', 'UW', 'V',\n",
    "    'W', 'Y', 'Z', 'ZH', 'SIL'\n",
    "]\n",
    "\n",
    "CHANG_PHONE_DEF = [\n",
    "    'AA', 'AE', 'AH', 'AW',\n",
    "    'AY', 'B',  'D', 'DH',\n",
    "    'EH', 'ER', 'EY', 'F', 'G',\n",
    "    'HH', 'IH', 'IY', 'K',\n",
    "    'L', 'M', 'N', 'NG', 'OW',\n",
    "    'P', 'R', 'S',\n",
    "    'T', 'TH', 'UH', 'UW', 'V',\n",
    "    'W', 'Y', 'Z'\n",
    "]\n",
    "\n",
    "CONSONANT_DEF = ['CH', 'SH', 'JH', 'R', 'B',\n",
    "                 'M',  'W',  'V',  'F', 'P',\n",
    "                 'D',  'N',  'L',  'S', 'T',\n",
    "                 'Z',  'TH', 'G',  'Y', 'HH',\n",
    "                 'K', 'NG', 'ZH', 'DH']\n",
    "VOWEL_DEF = ['EY', 'AE', 'AY', 'EH', 'AA',\n",
    "             'AW', 'IY', 'IH', 'OY', 'OW',\n",
    "             'AO', 'UH', 'AH', 'UW', 'ER']\n",
    "\n",
    "SIL_DEF = ['SIL']\n",
    "\n",
    "class SpeechDataset():\n",
    "    def __init__(self,\n",
    "                 rawFileDir,\n",
    "                 nInputFeatures,\n",
    "                 nClasses,\n",
    "                 maxSeqElements,\n",
    "                 bufferSize,\n",
    "                 syntheticFileDir=None,\n",
    "                 syntheticMixingRate=0.33,\n",
    "                 subsetSize=-1,\n",
    "                 labelDir=None,\n",
    "                 timeWarpSmoothSD=0.0,\n",
    "                 timeWarpNoiseSD=0.0,\n",
    "                 chanIndices=None\n",
    "                 ):\n",
    "\n",
    "        self.rawFileDir = rawFileDir\n",
    "        self.nInputFeatures = nInputFeatures\n",
    "        self.nClasses = nClasses\n",
    "        self.maxSeqElements = maxSeqElements\n",
    "        self.bufferSize = bufferSize\n",
    "        self.syntheticFileDir = syntheticFileDir\n",
    "        self.syntheticMixingRate = syntheticMixingRate\n",
    "        self.timeWarpSmoothSD = timeWarpSmoothSD\n",
    "        self.timeWarpNoiseSD = timeWarpNoiseSD\n",
    "        self.subsetSize = subsetSize\n",
    "        self.chanIndices = chanIndices\n",
    "        \n",
    "    def build(self, batchSize, isTraining):\n",
    "        def _loadDataset(fileDir):\n",
    "            files = sorted([str(x) for x in pathlib.Path(fileDir).glob(\"*.tfrecord\")])\n",
    "            if isTraining:\n",
    "                random.shuffle(files)\n",
    "\n",
    "            dataset = tf.data.TFRecordDataset(files)\n",
    "            return dataset\n",
    "\n",
    "        print(f'Load data from {self.rawFileDir}')\n",
    "        rawDataset = _loadDataset(self.rawFileDir)\n",
    "        if self.syntheticFileDir and self.syntheticMixingRate > 0:\n",
    "            print(f'Load data from {self.syntheticFileDir}')\n",
    "            syntheticDataset = _loadDataset(self.syntheticFileDir)\n",
    "            dataset = tf.data.experimental.sample_from_datasets(\n",
    "                [rawDataset.repeat(), syntheticDataset.repeat()],\n",
    "                weights=[1.0 - self.syntheticMixingRate, self.syntheticMixingRate])\n",
    "        else:\n",
    "            dataset = rawDataset\n",
    "\n",
    "        datasetFeatures = {\n",
    "            \"inputFeatures\": tf.io.FixedLenSequenceFeature([self.nInputFeatures], tf.float32, allow_missing=True),\n",
    "            #\"classLabelsOneHot\": tf.io.FixedLenSequenceFeature([self.nClasses+1], tf.float32, allow_missing=True),\n",
    "            \"newClassSignal\": tf.io.FixedLenSequenceFeature([], tf.float32, allow_missing=True),\n",
    "            \"ceMask\": tf.io.FixedLenSequenceFeature([], tf.float32, allow_missing=True),\n",
    "            \"seqClassIDs\": tf.io.FixedLenFeature((self.maxSeqElements), tf.int64),\n",
    "            \"nTimeSteps\": tf.io.FixedLenFeature((), tf.int64),\n",
    "            \"nSeqElements\": tf.io.FixedLenFeature((), tf.int64),\n",
    "            \"transcription\": tf.io.FixedLenFeature((self.maxSeqElements), tf.int64)\n",
    "        }\n",
    "\n",
    "        if self.timeWarpNoiseSD>0 and self.timeWarpSmoothSD>0:\n",
    "            from scipy.ndimage.filters import gaussian_filter1d\n",
    "            inp = np.zeros([200])\n",
    "            inp[int(len(inp)/2)] = 1\n",
    "            gaussKernel = gaussian_filter1d(inp, self.timeWarpSmoothSD)\n",
    "\n",
    "            validIdx = np.argwhere(gaussKernel>0.001)\n",
    "            gaussKernel = gaussKernel[validIdx]\n",
    "            gaussKernel = np.squeeze(gaussKernel/np.sum(gaussKernel))\n",
    "\n",
    "            timeWarpNoiseSD= self.timeWarpNoiseSD\n",
    "\n",
    "            def parseDatasetFunctionWarp(exampleProto):\n",
    "                dat = tf.io.parse_single_example(exampleProto, datasetFeatures)\n",
    "\n",
    "                warpDat = {}\n",
    "                warpDat['seqClassIDs'] = dat['seqClassIDs']\n",
    "                warpDat['nSeqElements'] = dat['nSeqElements']\n",
    "                warpDat['transcription'] = dat['transcription']\n",
    "\n",
    "                whiteNoise = tf.random.normal([dat['nTimeSteps']*2], mean=0, stddev=timeWarpNoiseSD)\n",
    "                rateNoise = tf.nn.conv1d(whiteNoise[tf.newaxis,:,tf.newaxis],\n",
    "                                         gaussKernel[:,np.newaxis,np.newaxis].astype(np.float32), 1, 'SAME')\n",
    "\n",
    "                rateNoise = rateNoise[0,:,0]\n",
    "                toSum = tf.ones([dat['nTimeSteps']*2], dtype=tf.float32) + rateNoise\n",
    "                toSum = tf.nn.relu(toSum)\n",
    "\n",
    "                warpFun = tf.cumsum(toSum)\n",
    "                resampleIdx = tf.cast(warpFun, dtype=tf.int32)\n",
    "                resampleIdx = resampleIdx[resampleIdx<tf.cast(dat['nTimeSteps'],dtype=tf.int32)]\n",
    "\n",
    "                warpDat['nTimeSteps'] = tf.cast(tf.reduce_sum(tf.cast(resampleIdx>-1,dtype=tf.int32)), dtype=tf.int32)\n",
    "                warpDat['inputFeatures'] = tf.gather(dat['inputFeatures'], resampleIdx, axis=0)\n",
    "                if self.chanIndices is not None:\n",
    "                    selectChans = tf.gather(warpDat['inputFeatures'], tf.constant(self.chanIndices),axis=-1)\n",
    "                    paddings = [[0, 0], [0, 256-tf.shape(selectChans)[-1]]]\n",
    "                    warpDat['inputFeatures'] = tf.pad(selectChans, paddings, 'CONSTANT',constant_values=0)\n",
    "                warpDat['newClassSignal'] = tf.gather(dat['newClassSignal'], resampleIdx, axis=0)\n",
    "                warpDat['ceMask'] = tf.gather(dat['ceMask'], resampleIdx, axis=0)\n",
    "\n",
    "                return warpDat\n",
    "\n",
    "            dataset = dataset.map(parseDatasetFunctionWarp, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "        else:\n",
    "            def parseDatasetFunctionSimple(exampleProto):\n",
    "                dat = tf.io.parse_single_example(exampleProto, datasetFeatures)\n",
    "                if self.chanIndices is not None:\n",
    "                    newDat = {}\n",
    "                    newDat['seqClassIDs'] = dat['seqClassIDs']\n",
    "                    newDat['nSeqElements'] = dat['nSeqElements']\n",
    "                    newDat['transcription'] = dat['transcription']\n",
    "                    newDat['nTimeSteps'] = dat['nTimeSteps']\n",
    "                    newDat['newClassSignal'] = dat['newClassSignal']\n",
    "                    newDat['ceMask'] = dat['ceMask']\n",
    "                    print(dat['inputFeatures'])\n",
    "                    selectChans = tf.gather(dat['inputFeatures'], tf.constant(self.chanIndices),axis=-1)\n",
    "                    paddings = [[0, 0], [0, 256-tf.shape(selectChans)[-1]]]\n",
    "                    newDat['inputFeatures'] = tf.pad(selectChans, paddings, 'CONSTANT',constant_values=0)\n",
    "                    print(tf.shape(newDat['inputFeatures']))\n",
    "\n",
    "                    return newDat\n",
    "                else:\n",
    "                    return dat\n",
    "            dataset = dataset.map(parseDatasetFunctionSimple, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "        if isTraining:\n",
    "            # Use all elements to adapt normalization layer\n",
    "            datasetForAdapt = dataset.map(lambda x: x['inputFeatures'] + 0.001,\n",
    "                num_parallel_calls=tf.data.AUTOTUNE)\n",
    "            \n",
    "            # Take a subset of the data if specified\n",
    "            if self.subsetSize > 0:\n",
    "                dataset = dataset.take(self.subsetSize)\n",
    "\n",
    "            # Shuffle and transform data if training\n",
    "            dataset = dataset.shuffle(self.bufferSize)\n",
    "            if self.syntheticMixingRate == 0:\n",
    "                dataset = dataset.repeat()\n",
    "            dataset = dataset.padded_batch(batchSize)\n",
    "            dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "            \n",
    "            \n",
    "\n",
    "            return dataset, datasetForAdapt\n",
    "        else:\n",
    "            dataset = dataset.padded_batch(batchSize)\n",
    "            dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "            return dataset\n",
    "\n",
    "\n",
    "def makeTFRecordsFromCompetitionFiles(sessionName, dataPath, tfRecordFolder):\n",
    "    \n",
    "    partNames = ['train','test','competitionHoldOut']\n",
    "    \n",
    "    for partIdx in range(len(partNames)):\n",
    "        sessionPath = dataPath + '/' + partNames[partIdx] + '/' + sessionName + '.mat'\n",
    "        if not os.path.isfile(sessionPath):\n",
    "            continue\n",
    "            \n",
    "        dat = scipy.io.loadmat(sessionPath)\n",
    "\n",
    "        input_features = []\n",
    "        transcriptions = []\n",
    "        frame_lens = []\n",
    "        block_means = []\n",
    "        block_stds = []\n",
    "        n_trials = dat['sentenceText'].shape[0]\n",
    "\n",
    "        #collect area 6v tx1 and spikePow features\n",
    "        for i in range(n_trials):    \n",
    "            #get time series of TX and spike power for this trial\n",
    "            #first 128 columns = area 6v only\n",
    "            features = np.concatenate([dat['tx1'][0,i][:,0:128], dat['spikePow'][0,i][:,0:128]], axis=1)\n",
    "\n",
    "            sentence_len = features.shape[0]\n",
    "            sentence = dat['sentenceText'][i].strip()\n",
    "\n",
    "            input_features.append(features)\n",
    "            transcriptions.append(sentence)\n",
    "            frame_lens.append(sentence_len)\n",
    "\n",
    "        #block-wise feature normalization\n",
    "        blockNums = np.squeeze(dat['blockIdx'])\n",
    "        blockList = np.unique(blockNums)\n",
    "        blocks = []\n",
    "        for b in range(len(blockList)):\n",
    "            sentIdx = np.argwhere(blockNums==blockList[b])\n",
    "            sentIdx = sentIdx[:,0].astype(np.int32)\n",
    "            blocks.append(sentIdx)\n",
    "\n",
    "        for b in range(len(blocks)):\n",
    "            feats = np.concatenate(input_features[blocks[b][0]:(blocks[b][-1]+1)], axis=0)\n",
    "            feats_mean = np.mean(feats, axis=0, keepdims=True)\n",
    "            feats_std = np.std(feats, axis=0, keepdims=True)\n",
    "            for i in blocks[b]:\n",
    "                input_features[i] = (input_features[i] - feats_mean) / (feats_std + 1e-8)\n",
    "\n",
    "        #convert to tfRecord file\n",
    "        session_data = {\n",
    "            'inputFeatures': input_features,\n",
    "            'transcriptions': transcriptions,\n",
    "            'frameLens': frame_lens\n",
    "        }\n",
    "\n",
    "        folderName = tfRecordFolder+'/'+partNames[partIdx]\n",
    "        convertToTFRecord(session_data, \n",
    "                          folderName,\n",
    "                          np.arange(0,len(input_features)).astype(np.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e88b8435",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getSpeechSessionBlocks import getSpeechSessionBlocks\n",
    "blockLists = getSpeechSessionBlocks()\n",
    "\n",
    "for sessIdx in range(len(blockLists)):\n",
    "    sessionName = blockLists[sessIdx][0]\n",
    "    dataPath = baseDir + '/competitionData'\n",
    "    tfRecordFolder = baseDir + '/derived/tfRecords/'+sessionName\n",
    "    makeTFRecordsFromCompetitionFiles(sessionName, dataPath, tfRecordFolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bc0c51",
   "metadata": {},
   "source": [
    "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
    "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
    "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
    "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
    "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
    "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
    " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
    " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
    " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
    " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
    " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
    " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
    " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
    " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
    " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
    " 270 271 272 273 274 275 276 277 278 279]\n",
    "['N', 'UW', 'K', 'L', 'IY', 'ER', 'SIL', 'R', 'AA', 'K', 'AH', 'T', 'S', 'SIL', 'K', 'AE', 'N', 'SIL', 'D', 'IH', 'S', 'T', 'R', 'OY', 'SIL', 'EH', 'R', 'F', 'IY', 'L', 'D', 'Z', 'SIL', 'W', 'IH', 'DH', 'SIL', 'IY', 'Z', 'SIL']\n",
    "nuclear rockets can destroy airfields with ease\n",
    "[23 34 20 21 18 12 40 28  1 20]\n",
    "['DH', 'AH', 'SIL', 'B', 'EH', 'S', 'T', 'SIL', 'W', 'EY', 'SIL', 'T', 'UW', 'SIL', 'L', 'ER', 'N', 'SIL', 'IH', 'Z', 'SIL', 'T', 'UW', 'SIL', 'S', 'AA', 'L', 'V', 'SIL', 'EH', 'K', 'S', 'T', 'R', 'AH', 'SIL', 'P', 'R', 'AA', 'B', 'L', 'AH', 'M', 'Z', 'SIL']\n",
    "the best way to learn is to solve extra problems\n",
    "[10  3 40  7 11 29 31 40 36 13]\n",
    "['DH', 'AH', 'SIL', 'S', 'P', 'R', 'EY', 'SIL', 'W', 'IH', 'L', 'SIL', 'B', 'IY', 'SIL', 'Y', 'UW', 'Z', 'D', 'SIL', 'IH', 'N', 'SIL', 'F', 'ER', 'S', 'T', 'SIL', 'D', 'IH', 'V', 'IH', 'ZH', 'AH', 'N', 'SIL', 'M', 'AE', 'CH', 'AH', 'Z', 'SIL', 'N', 'EH', 'K', 'S', 'T', 'SIL', 'S', 'IY', 'Z', 'AH', 'N', 'SIL']\n",
    "the spray will be used in first division matches next season\n",
    "[10  3 40 29 27 28 13 40 36 17]\n",
    "['AW', 'ER', 'SIL', 'IH', 'K', 'S', 'P', 'EH', 'R', 'AH', 'M', 'AH', 'N', 'T', 'S', 'SIL', 'P', 'AA', 'Z', 'AH', 'T', 'IH', 'V', 'SIL', 'AW', 'T', 'K', 'AH', 'M', 'SIL', 'W', 'AA', 'Z', 'SIL', 'AH', 'N', 'IH', 'K', 'S', 'P', 'EH', 'K', 'T', 'IH', 'D', 'SIL']\n",
    "our experiment's positive outcome was unexpected\n",
    "[ 5 12 40 17 20 29 27 11 28  3]\n",
    "['AE', 'L', 'AH', 'M', 'OW', 'N', 'IY', 'SIL', 'HH', 'AA', 'R', 'M', 'Z', 'SIL', 'AH', 'SIL', 'D', 'IH', 'V', 'AO', 'R', 'S', 'T', 'SIL', 'M', 'AE', 'N', 'Z', 'SIL', 'W', 'EH', 'L', 'TH', 'SIL']\n",
    "alimony harms a divorced man's wealth\n",
    "[ 2 21  3 22 25 23 18 40 16  1]\n",
    "['SH', 'IY', 'SIL', 'Y', 'UW', 'Z', 'IH', 'Z', 'SIL', 'B', 'OW', 'TH', 'SIL', 'N', 'EY', 'M', 'Z', 'SIL', 'IH', 'N', 'T', 'ER', 'CH', 'EY', 'N', 'JH', 'AH', 'B', 'L', 'IY', 'SIL']\n",
    "she uses both names interchangeably\n",
    "[30 18 40 37 34 38 17 38 40  7]\n",
    "['DH', 'AH', 'SIL', 'M', 'IH', 'S', 'K', 'W', 'OW', 'T', 'SIL', 'W', 'AA', 'Z', 'SIL', 'R', 'IY', 'T', 'R', 'AE', 'K', 'T', 'AH', 'D', 'SIL', 'W', 'IH', 'DH', 'SIL', 'AE', 'N', 'SIL', 'AH', 'P', 'AA', 'L', 'AH', 'JH', 'IY', 'SIL']\n",
    "the misquote was retracted with an apology\n",
    "[10  3 40 22 17 29 20 36 25 31]\n",
    "['K', 'R', 'IH', 'T', 'IH', 'K', 'S', 'SIL', 'F', 'IH', 'R', 'SIL', 'DH', 'AH', 'SIL', 'IH', 'R', 'OW', 'ZH', 'AH', 'N', 'SIL', 'AH', 'V', 'SIL', 'K', 'AH', 'N', 'S', 'UW', 'M', 'ER', 'SIL', 'P', 'R', 'AH', 'T', 'EH', 'K', 'SH', 'AH', 'N', 'Z', 'SIL', 'AH', 'N', 'D', 'SIL', 'IH', 'N', 'V', 'AY', 'R', 'AH', 'N', 'M', 'EH', 'N', 'T', 'AH', 'L', 'SIL', 'S', 'T', 'AE', 'N', 'D', 'ER', 'D', 'Z', 'SIL']\n",
    "critics fear the erosion of consumer protections and environmental standards\n",
    "[20 28 17 31 17 20 29 40 14 17]\n",
    "['HH', 'ER', 'SIL', 'L', 'IH', 'P', 'S', 'SIL', 'M', 'OY', 'S', 'T', 'SIL', 'AH', 'N', 'D', 'SIL', 'P', 'AA', 'R', 'T', 'AH', 'D', 'SIL', 'S', 'P', 'OW', 'K', 'SIL', 'HH', 'IH', 'Z', 'SIL', 'N', 'EY', 'M', 'SIL']\n",
    "her lips moist and parted spoke his name\n",
    "[16 12 40 21 17 27 29 40 22 26]\n",
    "['HH', 'AW', 'SIL', 'D', 'UW', 'SIL', 'DH', 'EY', 'SIL', 'T', 'ER', 'N', 'SIL', 'AW', 'T', 'SIL', 'L', 'EY', 'T', 'ER', 'SIL']\n",
    "how do they turn out later\n",
    "[16  5 40  9 34 40 10 13 40 31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10634913",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
