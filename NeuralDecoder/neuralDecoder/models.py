import tensorflow as tf
from tensorflow.keras import Model


class GRU(Model):
    def __init__(
        self,
        units,
        weightReg,
        actReg,
        subsampleFactor,
        nClasses,
        bidirectional=False,
        dropout=0.0,
        nLayers=2,
        conv_kwargs=None,
        stack_kwargs=None,
    ):
        super(GRU, self).__init__()

        weightReg = tf.keras.regularizers.L2(weightReg)
        # actReg = tf.keras.regularizers.L2(actReg)
        actReg = None
        recurrent_init = tf.keras.initializers.Orthogonal()
        kernel_init = tf.keras.initializers.glorot_uniform()
        self.subsampleFactor = subsampleFactor
        self.bidirectional = bidirectional
        self.stack_kwargs = stack_kwargs

        if bidirectional:
            self.initStates = [
                tf.Variable(initial_value=kernel_init(shape=(1, units))),
                tf.Variable(initial_value=kernel_init(shape=(1, units))),
            ]
        else:
            # gru
            # self.initStates = tf.Variable(initial_value=kernel_init(shape=(1, units)))
            # lstm / c & h state
            self.initStates_cell = tf.Variable(
                initial_value=kernel_init(shape=(1, units))
            )
            self.initStates_hidden = tf.Variable(
                initial_value=kernel_init(shape=(1, units))
            )

        self.conv1 = None
        if conv_kwargs is not None:
            self.conv1 = tf.keras.layers.DepthwiseConv1D(
                **conv_kwargs,
                padding="same",
                activation="relu",
                kernel_regularizer=weightReg,
                use_bias=False
            )

        self.rnnLayers = []
        for _ in range(nLayers):
            # GRU
            # rnn = tf.keras.layers.GRU(units,
            #                           return_sequences=True,
            #                           return_state=True,
            #                           kernel_regularizer=weightReg,
            #                           activity_regularizer=actReg,
            #                           recurrent_initializer=recurrent_init,
            #                           kernel_initializer=kernel_init,
            #                           dropout=dropout)

            # LSTM
            rnn = tf.keras.layers.LSTM(
                units,
                return_sequences=True,
                return_state=True,
                kernel_regularizer=weightReg,
                activity_regularizer=actReg,
                kernel_initializer="glorot_uniform",
                recurrent_initializer="orthogonal",
                dropout=dropout,
            )
            self.rnnLayers.append(rnn)
        if bidirectional:
            self.rnnLayers = [
                tf.keras.layers.Bidirectional(rnn) for rnn in self.rnnLayers
            ]
        self.dense = tf.keras.layers.Dense(nClasses)

    def call(self, x, states=None, training=False, returnState=False):
        batchSize = tf.shape(x)[0]

        if self.stack_kwargs is not None:
            x = tf.image.extract_patches(
                x[:, None, :, :],
                sizes=[1, 1, self.stack_kwargs["kernel_size"], 1],
                strides=[1, 1, self.stack_kwargs["strides"], 1],
                rates=[1, 1, 1, 1],
                padding="VALID",
            )
            x = tf.squeeze(x, axis=1)

        if self.conv1 is not None:
            x = self.conv1(x)

        # GRU
        # if states is None:
        #     states = []
        #     if self.bidirectional:
        #         states.append([tf.tile(s, [batchSize, 1]) for s in self.initStates])
        #     else:
        #         states.append(tf.tile(self.initStates, [batchSize, 1]))
        #     states.extend([None] * (len(self.rnnLayers) - 1))

        # LSTM
        if states is None:
            states = []
            if self.bidirectional:
                states.append([tf.tile(s, [batchSize, 1]) for s in self.initStates])
            else:
                state_cell = tf.tile(self.initStates_cell, [batchSize, 1])
                state_hidden = tf.tile(self.initStates_hidden, [batchSize, 1])
                states.append([state_cell, state_hidden])
            states.extend([None] * (len(self.rnnLayers) - 1))

        new_states = []
        if self.bidirectional:
            for i, rnn in enumerate(self.rnnLayers):
                x, forward_s, backward_s = rnn(
                    x, training=training, initial_state=states[i]
                )
                if i == len(self.rnnLayers) - 2:
                    if self.subsampleFactor > 1:
                        x = x[:, :: self.subsampleFactor, :]
                new_states.append([forward_s, backward_s])
        else:
            for i, rnn in enumerate(self.rnnLayers):
                # GRU
                # x, s = rnn(x, training=training, initial_state=states[i])
                # LSTM
                x, memory, carry = rnn(x, training=training, initial_state=states[i])
                if i == len(self.rnnLayers) - 2:
                    if self.subsampleFactor > 1:
                        x = x[:, :: self.subsampleFactor, :]
                new_states.append([memory, carry])

        x = self.dense(x, training=training)

        if returnState:
            return x, new_states
        else:
            return x

    # TODO: Fix me
    def getIntermediateLayerOutput(self, x):
        x, _ = self.rnn1(x)
        return x

    def getSubsampledTimeSteps(self, timeSteps):
        timeSteps = tf.cast(timeSteps / self.subsampleFactor, dtype=tf.int32)
        if self.stack_kwargs is not None:
            timeSteps = tf.cast(
                (timeSteps - self.stack_kwargs["kernel_size"])
                / self.stack_kwargs["strides"]
                + 1,
                dtype=tf.int32,
            )
        return timeSteps
